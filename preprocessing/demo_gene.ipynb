{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5310112-d96a-4e31-871e-086b8144b090",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deezer/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74eaf4-5e26-494d-9ee9-cfa731e2fd96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad37af76-07fe-4d9c-92d9-e5fccd277cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_DATA = \"../data/\"\n",
    "PATH_DATA = \"/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c87ad",
   "metadata": {},
   "source": [
    "## Loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c734eb88-f2f8-4d4d-b789-1daf7cf1a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 86 files.\n",
      "CPU times: user 45.4 s, sys: 522 ms, total: 45.9 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = load_corpus_from_json(PATH_DATA+\"aggregated_corpus.json\"\n",
    "                               , **{\"document_maker\":nltk_tokenizer_pipe\n",
    "                                    , \"annotated\":True\n",
    "                                    , \"label_name\":\"label\"\n",
    "                                    , \"label_pos\":\"DS\"\n",
    "                                    , \"label_neg\":\"O\"\n",
    "                                    , \"annotation_scheme\":None\n",
    "                                   }\n",
    "                              )\n",
    "\n",
    "print(\"Loaded corpus of {nb_files} files.\".format(nb_files=len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a1bb0b-4884-4428-bb5e-5b60f81f5844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 µs, sys: 0 ns, total: 172 µs\n",
      "Wall time: 179 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# by default, this methods splits subcorpora into <main> and <ood> as predefined\n",
    "main_corpus, corpus_ood = corpus.split_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225fb92",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c344677",
   "metadata": {},
   "source": [
    "For each of the files contained in the corpus, the labels are stored in terms of spans indexed on characters. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4890975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "DS segment number 0:\n",
      "\n",
      "— Cela sera-t-il bien long ?\n",
      "---\n",
      "DS segment number 1:\n",
      "\n",
      "— C'est l'affaire d'une heure,\n",
      "---\n",
      "DS segment number 2:\n",
      "\n",
      "nous allons nous mettre tout de suite à manger l'avoine.\n"
     ]
    }
   ],
   "source": [
    "example_file = main_corpus[\"Pauline\"]\n",
    "\n",
    "for i, ds_char_span in enumerate(example_file.char_spans_labels[:3]):\n",
    "    print(\"---\\nDS segment number {}:\\n\".format(i))\n",
    "    print(example_file.text[ds_char_span[0]:ds_char_span[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5488d",
   "metadata": {},
   "source": [
    "The text is tokenized and a `DataFrame` containing word-tokens and their corresponding labels is stored for each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c1420e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>sentstart</th>\n",
       "      <th>token_idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George</td>\n",
       "      <td>yes</td>\n",
       "      <td>(0, 6)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sand</td>\n",
       "      <td>no</td>\n",
       "      <td>(7, 11)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nouvelle</td>\n",
       "      <td>no</td>\n",
       "      <td>(12, 20)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ÉDition</td>\n",
       "      <td>no</td>\n",
       "      <td>(21, 28)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paris</td>\n",
       "      <td>no</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12913</th>\n",
       "      <td>de</td>\n",
       "      <td>no</td>\n",
       "      <td>(59822, 59824)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12914</th>\n",
       "      <td>toutes</td>\n",
       "      <td>no</td>\n",
       "      <td>(59825, 59831)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12915</th>\n",
       "      <td>ses</td>\n",
       "      <td>no</td>\n",
       "      <td>(59832, 59835)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12916</th>\n",
       "      <td>forces</td>\n",
       "      <td>no</td>\n",
       "      <td>(59836, 59842)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12917</th>\n",
       "      <td>.</td>\n",
       "      <td>no</td>\n",
       "      <td>(59842, 59843)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12918 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token sentstart       token_idx label\n",
       "0        George       yes          (0, 6)     O\n",
       "1          Sand        no         (7, 11)     O\n",
       "2      Nouvelle        no        (12, 20)     O\n",
       "3       ÉDition        no        (21, 28)     O\n",
       "4         Paris        no        (29, 34)     O\n",
       "...         ...       ...             ...   ...\n",
       "12913        de        no  (59822, 59824)     O\n",
       "12914    toutes        no  (59825, 59831)     O\n",
       "12915       ses        no  (59832, 59835)     O\n",
       "12916    forces        no  (59836, 59842)     O\n",
       "12917         .        no  (59842, 59843)     O\n",
       "\n",
       "[12918 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file.df_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18ea73",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc331f5b",
   "metadata": {},
   "source": [
    "The corpus can then be separated in Train/Val/Test splits, and their corresponding `DataFrame`s merged to be able to train ML models on these splits. `CorpusDict`'s method `merge_dfs_by_keys` will give a `dict` storing the merged `DataFrame`s according to a given segregating key and values. The name of the file is added in the table and \"EOF\" tags separate the different files aggregated in the `DataFrame`.\n",
    "\n",
    "See below the resulting tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ba117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df_splits = main_corpus.merge_dfs_by_keys(key=\"split\",\n",
    "                                               values=[\"train\", \"val\", \"test\"],\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe33798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>sentstart</th>\n",
       "      <th>token_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Madame</td>\n",
       "      <td>yes</td>\n",
       "      <td>(0, 6)</td>\n",
       "      <td>O</td>\n",
       "      <td>Madame_de_Hautefort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de</td>\n",
       "      <td>no</td>\n",
       "      <td>(7, 9)</td>\n",
       "      <td>O</td>\n",
       "      <td>Madame_de_Hautefort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hautefort</td>\n",
       "      <td>no</td>\n",
       "      <td>(10, 19)</td>\n",
       "      <td>O</td>\n",
       "      <td>Madame_de_Hautefort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n</td>\n",
       "      <td>no</td>\n",
       "      <td>(19, 20)</td>\n",
       "      <td>O</td>\n",
       "      <td>Madame_de_Hautefort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voici</td>\n",
       "      <td>no</td>\n",
       "      <td>(20, 25)</td>\n",
       "      <td>O</td>\n",
       "      <td>Madame_de_Hautefort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47433</th>\n",
       "      <td>fera</td>\n",
       "      <td>no</td>\n",
       "      <td>(211563, 211567)</td>\n",
       "      <td>DS</td>\n",
       "      <td>cousinebette_cecile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47434</th>\n",
       "      <td>faire</td>\n",
       "      <td>no</td>\n",
       "      <td>(211568, 211573)</td>\n",
       "      <td>DS</td>\n",
       "      <td>cousinebette_cecile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47435</th>\n",
       "      <td>fortune</td>\n",
       "      <td>no</td>\n",
       "      <td>(211574, 211581)</td>\n",
       "      <td>DS</td>\n",
       "      <td>cousinebette_cecile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47436</th>\n",
       "      <td>.</td>\n",
       "      <td>no</td>\n",
       "      <td>(211581, 211582)</td>\n",
       "      <td>DS</td>\n",
       "      <td>cousinebette_cecile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353694</th>\n",
       "      <td>EOF</td>\n",
       "      <td>EOF</td>\n",
       "      <td>EOF</td>\n",
       "      <td>EOF</td>\n",
       "      <td>EOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353695 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token sentstart         token_idx label                 file\n",
       "0          Madame       yes            (0, 6)     O  Madame_de_Hautefort\n",
       "1              de        no            (7, 9)     O  Madame_de_Hautefort\n",
       "2       Hautefort        no          (10, 19)     O  Madame_de_Hautefort\n",
       "3              \\n        no          (19, 20)     O  Madame_de_Hautefort\n",
       "4           Voici        no          (20, 25)     O  Madame_de_Hautefort\n",
       "...           ...       ...               ...   ...                  ...\n",
       "47433        fera        no  (211563, 211567)    DS  cousinebette_cecile\n",
       "47434       faire        no  (211568, 211573)    DS  cousinebette_cecile\n",
       "47435     fortune        no  (211574, 211581)    DS  cousinebette_cecile\n",
       "47436           .        no  (211581, 211582)    DS  cousinebette_cecile\n",
       "353694        EOF       EOF               EOF   EOF                  EOF\n",
       "\n",
       "[353695 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df_splits[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914287c",
   "metadata": {},
   "source": [
    "## Generating `.tsv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969a2327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 86 files.\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 353695 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/main_corpus/train.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 71745 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/main_corpus/val.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 62831 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/main_corpus/test.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " []\n",
      " composed of 0 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/ood_corpus/train.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " []\n",
      " composed of 0 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/ood_corpus/val.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 236682 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/nltk_tokenization/ood_corpus/test.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating files using the custom RegEx NLTK document tokenization\n",
    "\n",
    "generate_tokens_df(raw_data_path=PATH_DATA+\"aggregated_corpus.json\"\n",
    "                   ,output_dir=PATH_DATA+\"nltk_tokenization/\"\n",
    "                   ,**{\"document_maker\":nltk_tokenizer_pipe\n",
    "                       , \"annotated\":True\n",
    "                       , \"label_name\":\"label\"\n",
    "                       , \"label_pos\":\"DS\"\n",
    "                       , \"label_neg\":\"O\"\n",
    "                       , \"annotation_scheme\":None\n",
    "                      }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27d828b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 86 files.\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 333675 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/main_corpus/train.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 67574 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/main_corpus/val.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 59022 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/main_corpus/test.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " []\n",
      " composed of 0 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/ood_corpus/train.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " []\n",
      " composed of 0 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/ood_corpus/val.tsv\n",
      "\n",
      "Saving Dataframe with columns:\n",
      " ['token', 'sentstart', 'token_idx', 'label', 'file']\n",
      " composed of 222687 rows at:\n",
      "/src/data/nfs/analysis/NLP/audiobooks-nlu/direct-speech/final/spacy_tokenization/ood_corpus/test.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating files using the SpaCy document tokenization\n",
    "\n",
    "generate_tokens_df(raw_data_path=PATH_DATA+\"aggregated_corpus.json\"\n",
    "                   ,output_dir=PATH_DATA+\"spacy_tokenization/\"\n",
    "                   ,**{\"document_maker\":spacy_nlp\n",
    "                       , \"annotated\":True\n",
    "                       , \"label_name\":\"label\"\n",
    "                       , \"label_pos\":\"DS\"\n",
    "                       , \"label_neg\":\"O\"\n",
    "                       , \"annotation_scheme\":None\n",
    "                      }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecd78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
